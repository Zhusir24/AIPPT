"""
AI ç”Ÿæˆç›¸å…³ API ç«¯ç‚¹
"""

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
import json
import asyncio

from ....core.database import get_db
from ....core.logger import get_logger, log_async_function_call
from ....models.schemas import (
    OutlineGenerateRequest, OutlineGenerateResponse, BaseResponse, ContentGenerateRequest,
    APIConfigTestRequest, APIConfigTestResponse, APIConfigRequest
)
from ....services.ai_service import AIService
from ....services.template_service import TemplateService

router = APIRouter()
logger = get_logger(__name__)


@router.post("/generate-outline", response_model=OutlineGenerateResponse)
async def generate_outline(
    request: OutlineGenerateRequest,
    db: Session = Depends(get_db)
):
    """ç”Ÿæˆå†…å®¹å¤§çº²"""
    logger.info("ğŸ¯ å¼€å§‹ç”Ÿæˆå†…å®¹å¤§çº²")
    logger.debug(f"ğŸ“‹ è¯·æ±‚å‚æ•°: {request.dict()}")
    
    try:
        # åˆ›å»ºAIæœåŠ¡å®ä¾‹
        logger.debug("ğŸ¤– åˆ›å»º AI æœåŠ¡å®ä¾‹")
        service = AIService(db)
        
        # è®°å½•è¯·æ±‚è¯¦æƒ…
        logger.info(f"ğŸ“ ä¸»é¢˜: {request.topic}")
        logger.info(f"ğŸŒ è¯­è¨€: {request.language}")
        logger.info(f"ğŸ“ å¤§çº²é•¿åº¦: {request.outline_length}")
        if request.target_audience:
            logger.info(f"ğŸ‘¥ ç›®æ ‡å—ä¼—: {request.target_audience}")
        if request.presentation_duration:
            logger.info(f"â±ï¸ æ¼”ç¤ºæ—¶é•¿: {request.presentation_duration}")
        if request.additional_requirements:
            logger.info(f"ğŸ“‹ é¢å¤–è¦æ±‚: {request.additional_requirements}")
        
        # ç”Ÿæˆå¤§çº²
        logger.info("âš¡ è°ƒç”¨AIæœåŠ¡ç”Ÿæˆå¤§çº²")
        result = await service.generate_outline(request)
        
        logger.success("âœ… å¤§çº²ç”ŸæˆæˆåŠŸ")
        logger.debug(f"ğŸ“Š è¿”å›ç»“æœ: {result.dict()}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆå¤§çº²å¤±è´¥: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ç”Ÿæˆå¤§çº²å¤±è´¥: {str(e)}"
        )


@router.post("/generate-outline-stream")
async def generate_outline_stream(
    request: OutlineGenerateRequest,
    db: Session = Depends(get_db)
):
    """æµå¼ç”Ÿæˆå†…å®¹å¤§çº²ï¼ˆSSEï¼‰"""
    logger.info("ğŸŒŠ å¼€å§‹æµå¼ç”Ÿæˆå†…å®¹å¤§çº²")
    logger.debug(f"ğŸ“‹ è¯·æ±‚å‚æ•°: {request.dict()}")
    
    try:
        # åˆ›å»ºAIæœåŠ¡å®ä¾‹
        logger.debug("ğŸ¤– åˆ›å»º AI æœåŠ¡å®ä¾‹")
        service = AIService(db)
        
        # è®°å½•è¯·æ±‚è¯¦æƒ…
        logger.info(f"ğŸ“ ä¸»é¢˜: {request.topic}")
        logger.info(f"ğŸŒ è¯­è¨€: {request.language}")
        logger.info(f"ğŸ“ å¤§çº²é•¿åº¦: {request.outline_length}")
        
        chunk_count = 0
        
        async def event_generator():
            nonlocal chunk_count
            try:
                logger.info("âš¡ å¼€å§‹æµå¼ç”Ÿæˆ")
                async for chunk in service.generate_outline_stream(request):
                    chunk_count += 1
                    logger.debug(f"ğŸ“¦ å‘é€ç¬¬ {chunk_count} ä¸ªæ•°æ®å—: {chunk[:50]}...")
                    # æ„é€  SSE æ ¼å¼çš„æ•°æ®
                    yield f"data: {json.dumps({'content': chunk}, ensure_ascii=False)}\n\n"
                
                # å‘é€å®Œæˆäº‹ä»¶
                logger.info(f"âœ… æµå¼ç”Ÿæˆå®Œæˆï¼Œå…±å‘é€ {chunk_count} ä¸ªæ•°æ®å—")
                yield f"data: {json.dumps({'status': 'complete'}, ensure_ascii=False)}\n\n"
                
            except Exception as e:
                logger.error(f"âŒ æµå¼ç”Ÿæˆå¤±è´¥: {str(e)}")
                logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
                # å‘é€é”™è¯¯äº‹ä»¶
                error_data = {
                    'status': 'error',
                    'message': str(e)
                }
                yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
        
        logger.info("ğŸš€ å¼€å§‹è¿”å›æµå¼å“åº”")
        return StreamingResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ æµå¼ç”Ÿæˆå¤§çº²å¤±è´¥: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ç”Ÿæˆå¤§çº²å¤±è´¥: {str(e)}"
        )


@router.post("/generate-content")
async def generate_content(
    request: ContentGenerateRequest,
    db: Session = Depends(get_db)
):
    """æ ¹æ®å¤§çº²ç”Ÿæˆ PPT å†…å®¹"""
    logger.info("ğŸ“„ å¼€å§‹ç”Ÿæˆ PPT å†…å®¹")
    logger.debug(f"ğŸ“‹ è¯·æ±‚å‚æ•°: {request.dict()}")
    
    try:
        # åˆ›å»ºæœåŠ¡å®ä¾‹
        logger.debug("ğŸ¤– åˆ›å»º AI æœåŠ¡å®ä¾‹")
        ai_service = AIService(db)
        logger.debug("ğŸ“‹ åˆ›å»ºæ¨¡æ¿æœåŠ¡å®ä¾‹")
        template_service = TemplateService(db)
        
        # è·å–æ¨¡æ¿ä¿¡æ¯
        logger.info(f"ğŸ¨ è·å–æ¨¡æ¿ä¿¡æ¯ï¼Œæ¨¡æ¿ID: {request.template_id}")
        template = template_service.get_template(request.template_id)
        if not template:
            logger.warning(f"âŒ æ¨¡æ¿ä¸å­˜åœ¨: {request.template_id}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="æ¨¡æ¿ä¸å­˜åœ¨"
            )
        
        template_info = {
            "name": template.name,
            "description": template.description,
            "category": template.category
        }
        logger.info(f"âœ… æ¨¡æ¿ä¿¡æ¯è·å–æˆåŠŸ: {template.name} ({template.category})")
        logger.debug(f"ğŸ“‹ æ¨¡æ¿è¯¦æƒ…: {template_info}")
        
        # ç”Ÿæˆå†…å®¹
        logger.info("âš¡ è°ƒç”¨AIæœåŠ¡ç”Ÿæˆå†…å®¹")
        result = await ai_service.generate_content(request.outline, template_info)
        
        logger.success("âœ… PPTå†…å®¹ç”ŸæˆæˆåŠŸ")
        logger.debug(f"ğŸ“Š ç”Ÿæˆç»“æœ: {str(result)[:200]}...")
        
        return BaseResponse(
            success=True,
            message="å†…å®¹ç”ŸæˆæˆåŠŸ",
            data=result
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆå†…å®¹å¤±è´¥: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ç”Ÿæˆå†…å®¹å¤±è´¥: {str(e)}"
        )


@router.post("/generate-ppt")
async def generate_ppt(
    request: ContentGenerateRequest,
    db: Session = Depends(get_db)
):
    """ç”ŸæˆPPTæ–‡ä»¶"""
    logger.info("ğŸš€ğŸš€ğŸš€ PPTç”Ÿæˆè¯·æ±‚å·²åˆ°è¾¾åç«¯! ğŸš€ğŸš€ğŸš€")
    logger.info("ğŸ“Š å¼€å§‹ç”Ÿæˆå®Œæ•´ PPT æ–‡ä»¶")
    logger.info(f"ğŸ“‹ è¯·æ±‚å‚æ•°: outlineé•¿åº¦={len(request.outline)}, template_id={request.template_id}")
    logger.debug(f"ğŸ“‹ è¯¦ç»†è¯·æ±‚å‚æ•°: {request.dict()}")
    
    print("ğŸš€ğŸš€ğŸš€ PPTç”Ÿæˆè¯·æ±‚å·²åˆ°è¾¾åç«¯!")
    print(f"ğŸ“Š æ¨¡æ¿ID: {request.template_id}")
    print(f"ğŸ“„ å¤§çº²é•¿åº¦: {len(request.outline)} å­—ç¬¦")
    
    try:
        # åˆ›å»ºæœåŠ¡å®ä¾‹
        logger.debug("ğŸ¤– åˆ›å»º AI æœåŠ¡å®ä¾‹")
        ai_service = AIService(db)
        logger.debug("ğŸ“‹ åˆ›å»ºæ¨¡æ¿æœåŠ¡å®ä¾‹")
        template_service = TemplateService(db)
        
        # è·å–æ¨¡æ¿ä¿¡æ¯
        logger.info(f"ğŸ¨ è·å–æ¨¡æ¿ä¿¡æ¯ï¼Œæ¨¡æ¿ID: {request.template_id}")
        template = template_service.get_template(request.template_id)
        if not template:
            logger.warning(f"âŒ æ¨¡æ¿ä¸å­˜åœ¨: {request.template_id}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="æ¨¡æ¿ä¸å­˜åœ¨"
            )
        
        template_info = {
            "name": template.name,
            "description": template.description,
            "category": template.category
        }
        logger.info(f"âœ… æ¨¡æ¿ä¿¡æ¯è·å–æˆåŠŸ: {template.name} ({template.category})")
        
        # ç”Ÿæˆè¯¦ç»†å†…å®¹
        logger.info("âš¡ ç¬¬ä¸€æ­¥ï¼šç”ŸæˆPPTå†…å®¹")
        content_result = await ai_service.generate_content(request.outline, template_info)
        if content_result.get('error'):
            logger.error(f"âŒ å†…å®¹ç”Ÿæˆå¤±è´¥: {content_result['error']}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"å†…å®¹ç”Ÿæˆå¤±è´¥: {content_result['error']}"
            )
        logger.success("âœ… PPTå†…å®¹ç”Ÿæˆå®Œæˆ")
        
        # å¯¼å…¥PPTç”ŸæˆæœåŠ¡
        logger.debug("ğŸ“‹ å¯¼å…¥ PPTX æœåŠ¡")
        try:
            from ...services.pptx_service import PPTXService
            logger.debug("âœ… PPTX æœåŠ¡å¯¼å…¥æˆåŠŸ")
            pptx_service = PPTXService()
            logger.debug("âœ… PPTX æœåŠ¡å®ä¾‹åˆ›å»ºæˆåŠŸ")
        except ImportError as e:
            logger.error(f"âŒ PPTX æœåŠ¡å¯¼å…¥å¤±è´¥: {str(e)}")
            # å°è¯•å¦ä¸€ç§å¯¼å…¥æ–¹å¼
            try:
                import sys
                import os
                sys.path.append(os.path.join(os.path.dirname(__file__), '../../..'))
                from app.services.pptx_service import PPTXService
                logger.debug("âœ… PPTX æœåŠ¡å¯¼å…¥æˆåŠŸï¼ˆå¤‡ç”¨æ–¹å¼ï¼‰")
                pptx_service = PPTXService()
                logger.debug("âœ… PPTX æœåŠ¡å®ä¾‹åˆ›å»ºæˆåŠŸ")
            except Exception as e2:
                logger.error(f"âŒ PPTX æœåŠ¡å¯¼å…¥å½»åº•å¤±è´¥: {str(e2)}")
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=f"æ— æ³•å¯¼å…¥PPTç”ŸæˆæœåŠ¡: {str(e2)}"
                )
        
        # ç”ŸæˆPPTæ–‡ä»¶
        logger.info("âš¡ ç¬¬äºŒæ­¥ï¼šç”ŸæˆPPTæ–‡ä»¶")
        ppt_result = await pptx_service.create_presentation(
            outline=request.outline,
            content_data=content_result,
            template_info=template_info
        )
        
        logger.success(f"ğŸ‰ PPTæ–‡ä»¶ç”ŸæˆæˆåŠŸ: {ppt_result.get('filename', 'æœªçŸ¥æ–‡ä»¶å')}")
        logger.info(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {ppt_result.get('file_path', 'æœªçŸ¥è·¯å¾„')}")
        
        return BaseResponse(
            success=True,
            message="PPTç”ŸæˆæˆåŠŸ",
            data=ppt_result
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ PPTç”Ÿæˆå¤±è´¥: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        import traceback
        traceback.print_exc()  # æ‰“å°å®Œæ•´çš„é”™è¯¯å †æ ˆ
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"PPTç”Ÿæˆå¤±è´¥: {str(e)}"
        )


@router.get("/provider")
async def get_ai_provider(db: Session = Depends(get_db)):
    """è·å–å½“å‰AIæä¾›å•†ä¿¡æ¯"""
    logger.info("ğŸ¤– è·å–AIæä¾›å•†ä¿¡æ¯")
    
    try:
        logger.debug("ğŸ¤– åˆ›å»º AI æœåŠ¡å®ä¾‹")
        ai_service = AIService(db)
        
        logger.debug("ğŸ” è·å–å½“å‰æä¾›å•†")
        provider = ai_service.get_current_provider()
        
        logger.debug("ğŸ” è·å–å½“å‰æ¨¡å‹")
        model = ai_service._get_model_name()
        
        logger.info(f"âœ… AIæä¾›å•†: {provider}, æ¨¡å‹: {model}")
        
        return BaseResponse(
            success=True,
            message="è·å–AIæä¾›å•†ä¿¡æ¯æˆåŠŸ",
            data={
                "provider": provider,
                "model": model
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ è·å–AIæä¾›å•†ä¿¡æ¯å¤±è´¥: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–AIæä¾›å•†ä¿¡æ¯å¤±è´¥: {str(e)}"
        )


@router.post("/expand-content")
async def expand_content(
    section_title: str,
    current_content: str,
    db: Session = Depends(get_db)
):
    """æ‰©å±•å’Œä¸°å¯Œå†…å®¹"""
    logger.info("ğŸ“ å¼€å§‹æ‰©å±•å†…å®¹")
    logger.info(f"ğŸ“ ç« èŠ‚æ ‡é¢˜: {section_title}")
    logger.debug(f"ğŸ“‹ å½“å‰å†…å®¹é•¿åº¦: {len(current_content)} å­—ç¬¦")
    logger.debug(f"ğŸ“‹ å½“å‰å†…å®¹: {current_content[:100]}...")
    
    try:
        logger.debug("ğŸ¤– åˆ›å»º AI æœåŠ¡å®ä¾‹")
        service = AIService(db)
        
        logger.info("âš¡ è°ƒç”¨AIæœåŠ¡æ‰©å±•å†…å®¹")
        result = await service.expand_content(section_title, current_content)
        
        logger.success("âœ… å†…å®¹æ‰©å±•æˆåŠŸ")
        logger.debug(f"ğŸ“Š æ‰©å±•åå†…å®¹é•¿åº¦: {len(result)} å­—ç¬¦")
        logger.debug(f"ğŸ“Š æ‰©å±•ç»“æœ: {result[:100]}...")
        
        return BaseResponse(
            success=True,
            message="å†…å®¹æ‰©å±•æˆåŠŸ",
            data={"expanded_content": result}
        )
        
    except Exception as e:
        logger.error(f"âŒ æ‰©å±•å†…å®¹å¤±è´¥: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ‰©å±•å†…å®¹å¤±è´¥: {str(e)}"
        )


@router.get("/models")
async def get_available_models(db: Session = Depends(get_db)):
    """è·å–æ‰€æœ‰å¯ç”¨çš„AIæ¨¡å‹"""
    logger.info("ğŸ“‹ è·å–å¯ç”¨AIæ¨¡å‹åˆ—è¡¨")
    
    try:
        logger.debug("ğŸ¤– åˆ›å»º AI æœåŠ¡å®ä¾‹")
        service = AIService(db)
        
        logger.info("ğŸ” æŸ¥è¯¢å¯ç”¨æ¨¡å‹é…ç½®")
        models_info = service.get_available_models()
        
        logger.success(f"âœ… è·å–æ¨¡å‹åˆ—è¡¨æˆåŠŸï¼Œå…± {len(models_info['providers'])} ä¸ªæä¾›å•†")
        logger.info(f"ğŸ”— å½“å‰ä½¿ç”¨: {models_info['current_provider']} - {models_info['current_model']}")
        
        return BaseResponse(
            success=True,
            message="è·å–å¯ç”¨æ¨¡å‹æˆåŠŸ",
            data=models_info
        )
        
    except Exception as e:
        logger.error(f"âŒ è·å–å¯ç”¨æ¨¡å‹å¤±è´¥: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–å¯ç”¨æ¨¡å‹å¤±è´¥: {str(e)}"
        )


@router.post("/switch-model")
async def switch_model(
    provider: str,
    model: str,
    db: Session = Depends(get_db)
):
    """åˆ‡æ¢AIæ¨¡å‹"""
    logger.info(f"ğŸ”„ å¼€å§‹åˆ‡æ¢AIæ¨¡å‹: {provider} -> {model}")
    
    try:
        logger.debug("ğŸ¤– åˆ›å»º AI æœåŠ¡å®ä¾‹")
        service = AIService(db)
        
        logger.info(f"âš¡ æ‰§è¡Œæ¨¡å‹åˆ‡æ¢: {provider} / {model}")
        result = await service.switch_model(provider, model)
        
        if result["success"]:
            logger.success(f"ğŸ‰ æ¨¡å‹åˆ‡æ¢æˆåŠŸ: {result['old_provider']}/{result['old_model']} -> {result['new_provider']}/{result['new_model']}")
            logger.info(f"ğŸ§ª è¿æ¥æµ‹è¯•ç»“æœ: {result['test_result']['message']}")
            
            return BaseResponse(
                success=True,
                message="æ¨¡å‹åˆ‡æ¢æˆåŠŸ",
                data=result
            )
        else:
            logger.error(f"âŒ æ¨¡å‹åˆ‡æ¢å¤±è´¥: {result['message']}")
            return BaseResponse(
                success=False,
                message=result["message"],
                data=result
            )
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åˆ‡æ¢å¼‚å¸¸: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ¨¡å‹åˆ‡æ¢å¤±è´¥: {str(e)}"
        )


@router.post("/test-model")
async def test_current_model(db: Session = Depends(get_db)):
    """æµ‹è¯•å½“å‰æ¨¡å‹è¿æ¥"""
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•å½“å‰æ¨¡å‹è¿æ¥")
    
    try:
        logger.debug("ğŸ¤– åˆ›å»º AI æœåŠ¡å®ä¾‹")
        service = AIService(db)
        
        logger.info(f"ğŸ”— å½“å‰æ¨¡å‹: {service.get_current_provider()} - {service._get_model_name()}")
        logger.info("âš¡ æ‰§è¡Œè¿æ¥æµ‹è¯•")
        
        test_result = await service._test_model_connection()
        
        if test_result["success"]:
            logger.success(f"âœ… æ¨¡å‹è¿æ¥æµ‹è¯•æˆåŠŸ: {test_result['message']}")
            logger.debug(f"ğŸ“ æµ‹è¯•å“åº”: {test_result.get('test_response', 'æ— å“åº”')}")
        else:
            logger.error(f"âŒ æ¨¡å‹è¿æ¥æµ‹è¯•å¤±è´¥: {test_result['message']}")
        
        return BaseResponse(
            success=test_result["success"],
            message=test_result["message"],
            data={
                "current_provider": service.get_current_provider(),
                "current_model": service._get_model_name(),
                "test_result": test_result
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹æµ‹è¯•å¼‚å¸¸: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}"
        )


@router.post("/test-api-config", response_model=APIConfigTestResponse)
async def test_api_config(
    config: APIConfigTestRequest,
    db: Session = Depends(get_db)
):
    """æµ‹è¯•æŒ‡å®šçš„APIé…ç½®è¿æ¥"""
    logger.info(f"ğŸ§ª å¼€å§‹æµ‹è¯•APIé…ç½®è¿æ¥ - æä¾›å•†: {config.provider}")
    
    # è®°å½•è¯¦ç»†çš„é…ç½®ä¿¡æ¯ï¼ˆæ•æ„Ÿä¿¡æ¯å·²è„±æ•ï¼‰
    config_info = {
        "provider": config.provider,
        "api_key": f"{config.api_key[:8]}***{config.api_key[-4:]}" if config.api_key else "æœªè®¾ç½®",
        "custom_api_url": config.custom_api_url,
        "custom_model_name": config.custom_model_name
    }
    logger.info(f"ğŸ“‹ APIé…ç½®ä¿¡æ¯: {config_info}")
    
    try:
        import time
        import openai
        from openai import OpenAI
        
        start_time = time.time()
        
        # æ ¹æ®æä¾›å•†é…ç½®å®¢æˆ·ç«¯
        if config.provider == "openai":
            logger.info("ğŸ”— é…ç½®OpenAIå®¢æˆ·ç«¯")
            client = OpenAI(
                api_key=config.api_key,
                base_url="https://api.openai.com/v1"
            )
            model_name = "gpt-3.5-turbo"
            
        elif config.provider == "deepseek":
            logger.info("ğŸ”— é…ç½®DeepSeekå®¢æˆ·ç«¯")
            client = OpenAI(
                api_key=config.api_key,
                base_url="https://api.deepseek.com/v1"
            )
            model_name = "deepseek-chat"
            
        elif config.provider == "anthropic":
            logger.error("âŒ Anthropic æµ‹è¯•æš‚æœªå®ç°")
            return APIConfigTestResponse(
                success=False,
                message="Anthropic APIæµ‹è¯•åŠŸèƒ½æš‚æœªå®ç°",
                provider=config.provider,
                model_name=None
            )
            
        elif config.provider == "custom":
            logger.info(f"ğŸ”— é…ç½®è‡ªå®šä¹‰APIå®¢æˆ·ç«¯: {config.custom_api_url}")
            if not config.custom_api_url or not config.custom_model_name:
                logger.error("âŒ è‡ªå®šä¹‰APIé…ç½®ä¸å®Œæ•´")
                return APIConfigTestResponse(
                    success=False,
                    message="è‡ªå®šä¹‰APIé…ç½®éœ€è¦æä¾›APIåœ°å€å’Œæ¨¡å‹åç§°",
                    provider=config.provider,
                    model_name=config.custom_model_name
                )
            
            # éªŒè¯API URLæ ¼å¼
            import urllib.parse
            parsed_url = urllib.parse.urlparse(config.custom_api_url)
            if not parsed_url.scheme or not parsed_url.netloc:
                logger.error(f"âŒ æ— æ•ˆçš„API URLæ ¼å¼: {config.custom_api_url}")
                return APIConfigTestResponse(
                    success=False,
                    message=f"APIåœ°å€æ ¼å¼ä¸æ­£ç¡®: {config.custom_api_url}ã€‚è¯·ç¡®ä¿åŒ…å«å®Œæ•´çš„URLï¼ˆå¦‚: https://api.example.com/v1ï¼‰",
                    provider=config.provider,
                    model_name=config.custom_model_name
                )
            
            # åŸºæœ¬è¿é€šæ€§æµ‹è¯•
            try:
                import requests
                test_url = config.custom_api_url.rstrip('/') + '/models'
                logger.info(f"ğŸ§ª æµ‹è¯•APIç«¯ç‚¹è¿é€šæ€§: {test_url}")
                
                response = requests.get(
                    test_url,
                    headers={"Authorization": f"Bearer {config.api_key}"},
                    timeout=5
                )
                logger.info(f"ğŸ“¡ ç«¯ç‚¹å“åº”çŠ¶æ€: {response.status_code}")
            except requests.exceptions.ConnectionError:
                logger.warning("âš ï¸ æ— æ³•è¿æ¥åˆ°APIç«¯ç‚¹ï¼Œä½†å°†ç»§ç»­å°è¯•èŠå¤©æ¥å£æµ‹è¯•")
            except requests.exceptions.Timeout:
                logger.warning("âš ï¸ APIç«¯ç‚¹è¿æ¥è¶…æ—¶ï¼Œä½†å°†ç»§ç»­å°è¯•èŠå¤©æ¥å£æµ‹è¯•")
            except Exception as e:
                logger.warning(f"âš ï¸ ç«¯ç‚¹æµ‹è¯•å¼‚å¸¸: {str(e)}ï¼Œä½†å°†ç»§ç»­å°è¯•èŠå¤©æ¥å£æµ‹è¯•")
            
            client = OpenAI(
                api_key=config.api_key,
                base_url=config.custom_api_url
            )
            model_name = config.custom_model_name
        
        else:
            logger.error(f"âŒ ä¸æ”¯æŒçš„æä¾›å•†: {config.provider}")
            return APIConfigTestResponse(
                success=False,
                message=f"ä¸æ”¯æŒçš„APIæä¾›å•†: {config.provider}",
                provider=config.provider,
                model_name=None
            )
        
        # æ‰§è¡Œæµ‹è¯•è¯·æ±‚
        logger.info(f"âš¡ å‘é€æµ‹è¯•è¯·æ±‚åˆ°æ¨¡å‹: {model_name}")
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "user", "content": config.test_message}
                ],
                max_tokens=50,
                timeout=10
            )
        except openai.NotFoundError as e:
            logger.error(f"âŒ æ¨¡å‹æœªæ‰¾åˆ° (404é”™è¯¯): {model_name}")
            logger.error(f"ğŸ’¡ å¯èƒ½çš„åŸå› :")
            logger.error(f"   1. æ¨¡å‹åç§°ä¸æ­£ç¡®ï¼Œè¯·æ£€æŸ¥æ‹¼å†™")
            logger.error(f"   2. APIæœåŠ¡ä¸æ”¯æŒè¯¥æ¨¡å‹")
            logger.error(f"   3. APIç«¯ç‚¹URLé…ç½®é”™è¯¯")
            
            # æä¾›å¸¸è§æ¨¡å‹åç§°å»ºè®®
            model_suggestions = []
            if "qwen" in model_name.lower():
                model_suggestions = ["qwen2.5-coder-plus", "qwen-plus", "qwen2.5-plus", "qwen-max"]
            elif "gpt" in model_name.lower():
                model_suggestions = ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"]
            elif "claude" in model_name.lower():
                model_suggestions = ["claude-3-haiku", "claude-3-sonnet", "claude-3-opus"]
            
            suggestion_text = ""
            if model_suggestions:
                suggestion_text = f"\nğŸ’¡ å»ºè®®å°è¯•çš„æ¨¡å‹åç§°: {', '.join(model_suggestions)}"
            
            return APIConfigTestResponse(
                success=False,
                message=f"æ¨¡å‹ '{model_name}' æœªæ‰¾åˆ° (404é”™è¯¯)ã€‚è¯·æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ï¼Œæˆ–ç¡®è®¤APIæœåŠ¡æ”¯æŒè¯¥æ¨¡å‹ã€‚{suggestion_text}",
                provider=config.provider,
                model_name=model_name
            )
        except openai.AuthenticationError as e:
            logger.error(f"âŒ APIè®¤è¯å¤±è´¥: {str(e)}")
            return APIConfigTestResponse(
                success=False,
                message="APIå¯†é’¥è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®",
                provider=config.provider,
                model_name=model_name
            )
        except openai.PermissionDeniedError as e:
            logger.error(f"âŒ APIæƒé™è¢«æ‹’ç»: {str(e)}")
            return APIConfigTestResponse(
                success=False,
                message="APIè®¿é—®æƒé™è¢«æ‹’ç»ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥æƒé™è®¾ç½®",
                provider=config.provider,
                model_name=model_name
            )
        except openai.APIConnectionError as e:
            logger.error(f"âŒ APIè¿æ¥å¤±è´¥: {str(e)}")
            return APIConfigTestResponse(
                success=False,
                message=f"æ— æ³•è¿æ¥åˆ°APIæœåŠ¡å™¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIåœ°å€é…ç½®: {config.custom_api_url}",
                provider=config.provider,
                model_name=model_name
            )
        
        end_time = time.time()
        latency = round((end_time - start_time) * 1000, 2)  # æ¯«ç§’
        
        response_text = response.choices[0].message.content
        logger.success(f"âœ… APIé…ç½®æµ‹è¯•æˆåŠŸ - å»¶è¿Ÿ: {latency}ms")
        logger.debug(f"ğŸ“ æµ‹è¯•å“åº”: {response_text[:100]}...")
        
        return APIConfigTestResponse(
            success=True,
            message=f"è¿æ¥æˆåŠŸï¼æ¨¡å‹å“åº”æ­£å¸¸ï¼Œå»¶è¿Ÿ {latency}ms",
            provider=config.provider,
            model_name=model_name,
            response_preview=response_text[:100] + "..." if len(response_text) > 100 else response_text,
            latency=latency
        )
        
    except openai.AuthenticationError as e:
        logger.error(f"âŒ APIè®¤è¯å¤±è´¥: {str(e)}")
        return APIConfigTestResponse(
            success=False,
            message=f"APIå¯†é’¥è®¤è¯å¤±è´¥: {str(e)}",
            provider=config.provider,
            model_name=model_name if 'model_name' in locals() else None
        )
        
    except openai.APIConnectionError as e:
        logger.error(f"âŒ APIè¿æ¥å¤±è´¥: {str(e)}")
        return APIConfigTestResponse(
            success=False,
            message=f"æ— æ³•è¿æ¥åˆ°APIæœåŠ¡å™¨: {str(e)}",
            provider=config.provider,
            model_name=model_name if 'model_name' in locals() else None
        )
        
    except openai.RateLimitError as e:
        logger.error(f"âŒ APIè¯·æ±‚é™åˆ¶: {str(e)}")
        return APIConfigTestResponse(
            success=False,
            message=f"APIè¯·æ±‚é¢‘ç‡é™åˆ¶: {str(e)}",
            provider=config.provider,
            model_name=model_name if 'model_name' in locals() else None
        )
        
    except Exception as e:
        logger.error(f"âŒ APIé…ç½®æµ‹è¯•å¼‚å¸¸: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return APIConfigTestResponse(
            success=False,
            message=f"æµ‹è¯•å¤±è´¥: {str(e)}",
            provider=config.provider,
            model_name=model_name if 'model_name' in locals() else None
        )


@router.post("/save-api-config")
async def save_api_config(
    config: APIConfigRequest,
    db: Session = Depends(get_db)
):
    """ä¿å­˜APIé…ç½®è®¾ç½®"""
    logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜APIé…ç½® - æä¾›å•†: {config.provider}")
    
    # è®°å½•è¯¦ç»†çš„é…ç½®ä¿¡æ¯ï¼ˆæ•æ„Ÿä¿¡æ¯å·²è„±æ•ï¼‰
    config_info = {
        "provider": config.provider,
        "api_key": f"{config.api_key[:8]}***{config.api_key[-4:]}" if config.api_key else "æœªè®¾ç½®",
        "custom_api_url": config.custom_api_url,
        "custom_model_name": config.custom_model_name
    }
    logger.info(f"ğŸ“‹ ä¿å­˜çš„APIé…ç½®: {config_info}")
    
    try:
        import time
        
        # ä¿å­˜é…ç½®å¹¶ç«‹å³åº”ç”¨
        logger.info("ğŸ”„ ä¿å­˜é…ç½®å¹¶ç«‹å³åˆ‡æ¢åˆ°æ–°çš„APIé…ç½®")
        
        # åˆ›å»ºAIæœåŠ¡å®ä¾‹æ¥åº”ç”¨é…ç½®
        service = AIService(db)
        
        # æ ¹æ®æä¾›å•†ç±»å‹ç¡®å®šåˆ‡æ¢å‚æ•°
        if config.provider == "custom":
            if not config.custom_api_url or not config.custom_model_name:
                raise ValueError("è‡ªå®šä¹‰APIé…ç½®éœ€è¦æä¾›APIåœ°å€å’Œæ¨¡å‹åç§°")
            
            # ä¸ºè‡ªå®šä¹‰é…ç½®è®¾ç½®ç‰¹æ®Šçš„provideråç§°å’Œæ¨¡å‹åç§°
            provider_name = f"custom-{config.custom_api_url}"
            model_name = config.custom_model_name
            
            # ä¸´æ—¶è®¾ç½®è‡ªå®šä¹‰é…ç½®åˆ°æœåŠ¡ä¸­
            logger.info(f"ğŸ› ï¸ åº”ç”¨è‡ªå®šä¹‰APIé…ç½®: {config.custom_api_url} / {model_name}")
            service._apply_custom_config(config.api_key, config.custom_api_url, model_name)
            
        elif config.provider == "openai":
            provider_name = "OpenAI"
            model_name = "gpt-3.5-turbo"
            service._apply_openai_config(config.api_key)
            
        elif config.provider == "deepseek":
            provider_name = "DeepSeek"
            model_name = "deepseek-chat"
            service._apply_deepseek_config(config.api_key)
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„APIæä¾›å•†: {config.provider}")
        
        logger.success(f"âœ… APIé…ç½®ä¿å­˜å¹¶åº”ç”¨æˆåŠŸ - æä¾›å•†: {config.provider}")
        logger.info(f"ğŸ¯ å½“å‰ä½¿ç”¨: {provider_name} / {model_name}")
        
        return BaseResponse(
            success=True,
            message=f"APIé…ç½®ä¿å­˜æˆåŠŸï¼Œå·²åˆ‡æ¢åˆ° {provider_name} ({model_name})",
            data={
                "provider": provider_name,
                "model": model_name,
                "timestamp": time.time(),
                "config_saved": True,
                "applied": True
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜APIé…ç½®å¤±è´¥: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ä¿å­˜APIé…ç½®å¤±è´¥: {str(e)}"
        ) 